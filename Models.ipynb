{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The **Lagrangian** $ L(\\phi, \\dot{\\phi}, t) $ describes the motion of a system, where:\n",
    "- $ \\phi $ = Vertical angle of Pendulum from support\n",
    "- $ \\dot{\\phi} $ = Angular velocity of Pendulum from support\n",
    "- $t $ = Time variable\n",
    "\n",
    "The Euler-Lagrange equation is:\n",
    "$$\n",
    "\\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot{\\phi}_j} = \\frac{\\partial L}{\\partial \\phi_j}\n",
    "$$\n",
    "This can be written as:\n",
    "$$\n",
    "\\frac{d}{dt} \\nabla_{\\dot{\\phi}} L = \\nabla_\\phi L\n",
    "$$\n",
    "where:\n",
    "- $ \\nabla_\\phi L $ is the gradient w.r.t. $ \\phi $\n",
    "- $ \\nabla_{\\dot{\\phi}} L $ is the gradient w.r.t. $ \\dot{\\phi} $\n",
    "\n",
    "To take the time derivative of $ \\nabla_{\\dot{\\phi}} L $, use the chain rule:\n",
    "$$\n",
    "\\frac{d}{dt} \\nabla_{\\dot{\\phi}} L = \\frac{\\partial}{\\partial t} \\nabla_{\\dot{\\phi}} L + \\nabla^2_{\\dot{\\phi} \\dot{\\phi}} L \\, \\ddot{\\phi} + \\nabla^2_{\\phi \\dot{\\phi}} L \\, \\dot{\\phi}\n",
    "$$\n",
    "where:\n",
    "- $ \\nabla^2_{\\dot{\\phi} \\dot{\\phi}} L $ is the **Hessian w.r.t. $ \\dot{\\phi} $** (an $ n \\times n $ matrix)\n",
    "- $ \\nabla^2_{\\phi \\dot{\\phi}} L $ is the mixed partial derivative matrix\n",
    "\n",
    "This reflects that $ L $ can depend arbitrarily on both $ \\phi $ and $ \\dot{\\phi} $.\n",
    "\n",
    "You want to solve for $ \\ddot{\\phi} $:\n",
    "$$\n",
    "\\frac{d}{dt} \\nabla_{\\dot{\\phi}} L = \\nabla_\\phi L\n",
    "$$\n",
    "Plug in the expanded chain rule:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial t} \\nabla_{\\dot{\\phi}} L + \\nabla^2_{\\dot{\\phi} \\dot{\\phi}} L \\, \\ddot{\\phi} + \\nabla^2_{\\phi \\dot{\\phi}} L \\, \\dot{\\phi} = \\nabla_\\phi L\n",
    "$$\n",
    "Rearrange:\n",
    "$$\n",
    "\\nabla^2_{\\dot{\\phi} \\dot{\\phi}} L \\, \\ddot{\\phi} = \\nabla_\\phi L - \\nabla^2_{\\phi \\dot{\\phi}} L \\, \\dot{\\phi} - \\frac{\\partial}{\\partial t} \\nabla_{\\dot{\\phi}} L\n",
    "$$\n",
    "Multiply both sides by the inverse (if it exists) of the Hessian $ \\nabla^2_{\\dot{\\phi} \\dot{\\phi}} L $:\n",
    "$$\n",
    "\\ddot{\\phi} = [\\nabla^2_{\\dot{\\phi} \\dot{\\phi}} L]^{-1} [\\nabla_\\phi L - \\nabla^2_{\\phi \\dot{\\phi}} L \\, \\dot{\\phi} - \\frac{\\partial}{\\partial t} \\nabla_{\\dot{\\phi}} L]\n",
    "$$\n",
    "\n"
   ],
   "id": "cedb37881ec2dc06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T14:09:38.403319Z",
     "start_time": "2025-08-15T14:09:37.710473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "id": "74ed1c97a7a10066",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LNN_SoftPlus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LNN_SoftPlus, self).__init__()\n",
    "        self.Linear_Stack = nn.Sequential(\n",
    "            nn.Linear(3, 128),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, phi, phi_dot, gamma , t):\n",
    "        state = torch.cat([phi, phi_dot, t], dim=-1)\n",
    "        return self.Linear_Stack(state).squeeze(-1)\n"
   ],
   "id": "45bc261d2ecc853"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def Euler_Lagrange(phi, phi_dot, t, L_fn):\n",
    "    phi.requires_grad(True)\n",
    "    phi_dot.requires_grad(True)\n",
    "    t.requires_grad(True)\n",
    "    L = L_fn(phi, phi_dot, t)\n",
    "    grad_phi = torch.autograd.grad(L, phi, create_graph= True, retain_graph=True)\n",
    "    grad_phi_dot = torch.autograd.grad(L, phi_dot,  create_graph= True, retain_graph=True)\n",
    "\n",
    "    Hess = torch.autograd.functional.hessian(\n",
    "        lambda pd: L_fn(phi, pd, t), phi_dot,  create_graph= True)\n",
    "    Jaco = torch.autograd.functional.jacobian(\n",
    "        lambda p: torch.autograd.grad(L_fn(p, phi_dot, t), phi_dot, create_graph= True, retain_graph= True),\n",
    "        phi, create_graph=True)\n",
    "\n",
    "    TimeJac = torch.autograd.functional.jacobian(\n",
    "        lambda tt: torch.autograd.grad(L_fn(phi, phi_dot, tt), phi_dot, create_graph=True, retain_graph=True),\n",
    "        t, create_graph=True\n",
    "    )\n",
    "\n",
    "    if TimeJac.shape[-1] == 1:\n",
    "        TimeJac = TimeJac.squeeze(-1)\n",
    "\n",
    "    phi_ddot = torch.linalg.solve(Hess, grad_phi - Jaco @ phi_dot - TimeJac)\n",
    "    return phi_ddot\n"
   ],
   "id": "9d41aab0bc26e5fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def rk4(func, state, time, dt):\n",
    "    k1 = func(state, time)\n",
    "    k2 = func(state + 0.5 * dt * k1, state + 0.5 * dt)\n",
    "    k3 = func(state + 0.5 * dt * k2, state + 0.5 * dt)\n",
    "    k4 = func(state + dt * k3, state + dt)\n",
    "    return y + (dt/6) * (k1 + 2*k2 + 2*k3 + k4)"
   ],
   "id": "ab9a848b9671cc7f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
